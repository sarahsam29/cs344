{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Problem 1: Spam filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A Spam filter based on probability, heavily inspired by the article \"A Plan for Spam\" by Paul Graham\n",
    "\n",
    "@author: ss63\n",
    "@date: March 8, 2019\n",
    "'''\n",
    "\n",
    "\n",
    "spam_corpus = [[\"i\", \"am\", \"spam\", \"spam\", \"i\", \"am\"], [\"i\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "notspam_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "def SpamFilter(spam_corpus, notspam_corpus):\n",
    "\n",
    "    good_words = {}     #1st dictonary to store the words that are not in spam\n",
    "    num_good = 0        #will decide the length of goof_words dict.\n",
    "    for i in notspam_corpus:\n",
    "        num_good +=1\n",
    "        for n in i:\n",
    "            if n not in good_words:\n",
    "                good_words[n] = 1\n",
    "            else:\n",
    "                good_words[n] += 1\n",
    "\n",
    "    bad_words = {}       #2nd dictonary to store the words that are not in spam\n",
    "    num_bad = 0          #will decide the length of goof_words dict.\n",
    "    for i in spam_corpus:\n",
    "        num_bad += 1\n",
    "        for n in i:\n",
    "            if n not in bad_words:\n",
    "                bad_words[n] = 1\n",
    "            else:\n",
    "                bad_words[n] += 1\n",
    "\n",
    "    all_words = []       #create a list that combines both corpuses together\n",
    "    for token in good_words:\n",
    "        if token not in all_words:\n",
    "            all_words.append(token)\n",
    "    for token in bad_words:\n",
    "        if token not in all_words:\n",
    "            all_words.append(token)\n",
    "\n",
    "    corpus_prob = {}    #3rd dictionary to calculate the probability of each word, to eventually figure out if the word belongs to a spam email\n",
    "    for token in all_words:\n",
    "        g = 0;\n",
    "        b= 0;\n",
    "        if token in good_words:\n",
    "            g = 2 * good_words[token]\n",
    "        if token in bad_words:\n",
    "            b = bad_words[token]\n",
    "        probability = max(0.01, min(0.99, min(1.0, b /num_bad) / (min(1.0, g /num_good) + min(1.0, b /num_bad))))\n",
    "        corpus_prob[token] = probability\n",
    "\n",
    "    print(\"Combined corpus words and their probability:\")\n",
    "    return corpus_prob\n",
    "\n",
    "\n",
    "#figure out the probability that email is spam by checking each word's probability of belonging to spam email\n",
    "def checkForSpam(text):\n",
    "    product = 1\n",
    "    complement_product = 1\n",
    "\n",
    "    for token in text:\n",
    "        if token in sort:\n",
    "            product *= sort[token]\n",
    "            complement_product  *= 1 - sort[token]\n",
    "\n",
    "        else:\n",
    "            sort[token] = 0.4\n",
    "\n",
    "        prob_of_spam = product / (product + complement_product )\n",
    "    return prob_of_spam\n",
    "\n",
    "sort = SpamFilter(spam_corpus, notspam_corpus)\n",
    "print(sort)\n",
    "\n",
    "\n",
    "# Test cases using words from both corpuses\n",
    "spam1 = [\"I\", \"do\", \"not\", \"like\", \"spam\"]\n",
    "spam2 =  [\"I\", \"am\", \"spam\", \"that\", \"I\", \"am\"]\n",
    "\n",
    "notspam1 = [\"I\", \"dont\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"]\n",
    "notspam2 =  [\"I\", \"love\", \"green\", \"eggs\", \"not\", \"spam\"]\n",
    "]\n",
    "print(\"\\nProbability that 'I do not like spam' is spam:\", checkForSpam(spam1))\n",
    "print(\"\\nProbability that 'I am spam that I am' is spam:\",checkForSpam(spam2))\n",
    "print(\"\\nProbability that 'I dont like green eggs' is spam:\",checkForSpam(notspam1))\n",
    "print(\"\\nProbability that 'I do love green eggs not spam:\",checkForSpam(notspam2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe this approach is Bayesian because of the probabilities are conditional. A word being spam is calculated based on the prior belief that it is spam (based on if the word appears or doesn't appear in the spam_corpus). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Problem 2: Bayesian Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This module implements the Bayesian network shown in Exercise 5.3 of lab05\n",
    "It's taken from the AIMA Python code.\n",
    "\n",
    "@author: kvlinden, and ss63\n",
    "@version updated on March 4, 2019\n",
    "'''\n",
    "\n",
    "from probability import BayesNet, enumeration_ask, elimination_ask\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "cloudy = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T: 0.10, F: 0.50}),\n",
    "    ('Rain', 'Cloudy', {T: 0.80, F: 0.20}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T): 0.99, (T, F):0.90, (F, T):0.90, (F, F):0.00})\n",
    "    ])\n",
    "\n",
    "print(\"\\nP(Cloudy):\")\n",
    "print( enumeration_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Sprinkler|cloudy):\")\n",
    "print( enumeration_ask('Sprinkler', dict(Cloudy = T), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Cloudy|sprinkler and -rain):\")\n",
    "print( enumeration_ask('Cloudy', dict(Sprinkler = T, Rain = F), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Wet Grass|cloudy and sprinkler and rain):\")\n",
    "print(enumeration_ask('WetGrass', dict(Cloudy = T, Sprinkler = T, Rain = T), cloudy).show_approx())\n",
    "\n",
    "print(\"\\nP(Cloudy|- wet grass):\")\n",
    "print( enumeration_ask('Cloudy', dict(WetGrass = F), cloudy).show_approx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part b) Because there are 4 variables and T/F values for each, the number of independent values are 2^4 = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part c) I counted the multiply connected network to record 9. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
