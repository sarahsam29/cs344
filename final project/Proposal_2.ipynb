{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Proposal (pt.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarah Samuel, CS344  \n",
    "April 19, 2019  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good starting point in the Unstructured Text Data Project is to understand the primary data set which I will be utilizing. Below you will see the first few rows of the “historicalData5years” dataset that I have obtained from Service Express’ database. I will be using the TcaProblem field, which contains information about customers’ equipment that needs servicing, and the TcaPart field, which indicates the hardware part used to repair the issue. My plan is to develop a program that can accurately predict a hardware part when a new problem comes in. The main purpose is to maximize the efficiency of Service Express’ Field Service Engineers’ operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "uta_df = pd.read_csv(\"historicalData5years.csv\")\n",
    "uta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe the data by observing the shape, size, and variable types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1254, 4) size: 5016\n"
     ]
    }
   ],
   "source": [
    "shape = uta_df.shape\n",
    "size = uta_df.size\n",
    "\n",
    "print(\"shape:\", shape, \"size:\", size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TcaPkey              int64\n",
       "TcaCompletedDate    object\n",
       "TcaProblem          object\n",
       "Part                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uta_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation:   \n",
    "I am essentially creating a supervised machine learning model that implements text classification. This model uses “closed” tickets (tickets/rows which have been repaired) to learn about how the unstructured words in the TcaProblem field are associated with the parts in the TcaPart field. I am tokenizing the TcaProblem field words into n-grams, using python’s nltk (natural language tool kit) package. N-grams are a connected sequence of n-tokens, where n can be a sensible positive integer.  For my model, it makes sense to have n be less than 5, as TcaProblems are short descriptions and do not contain full-fledged sentences. I was able to gain context about n-grams from a thorough blog post written by Alvin Au Yeung: http://www.albertauyeung.com/post/generating-ngrams-python/. Additionally, this article written by Tan, Wang, and Lee explains how n-gram logic is rooted in the Naive Bayes approach:https://www.sciencedirect.com/science/article/abs/pii/S0306457301000450. \n",
    "I am utilizing n-gram functionality to assign frequencies to words as they associate with hardware parts. From my research, I am noticing that text classification is often implemented with nueral networks that split the data into training, testing and validation sets. Here is a link that does use nueral networks: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/. While this link has helped me gain more context, and also addresses n-grams, my project will not use nueral networks. Instead, it resembles Paul Graham's \"a Plan for Spam\" supervised learning model.  \n",
    "\n",
    "Background and anticipated results:  \n",
    "I found the need to use n-grams when I realized that the sequences of words in the TcaProblem field affected the frequency of words, and therefore the associations between words and parts. I came to this realization as I completed the first part of this project in SQL that used uni-grams. For example, the words “failed” and “bad” are commonly used to describe a hard-drive, or hard-disk. Moreover, Service Express repairs more hard-drives than any other hardware part. This means that there are more instances of hard-drive in the dataset, than any other hardware part. Due to these reasons, the results are very skewed when the TcaProblem fields include particular words like “failed” and “bad”. I am hoping that n-grams will be able to capture the words of TcaProblem in context now that it can tokenize the sentence into 2, 3 or n chunks. With n-grams, the associations will no longer occur on a single token, rather they will occur on tokens that have more information since it contains 2-3 contiguous words in a single string. If the TcaProblem says “my fan failed” I am expecting to see my program not so readily suggest hard-drive, but instead suggest a more sensible part, such as a fan assy.    \n",
    "\n",
    "Ethical Implications:  \n",
    "When I first began this project for Service Express, and had little information about Feild Engineers’ job, I was worried that I was automating their job which would slowly lead to lays offs. This is definitely not the case. Instead, I am streamlining tedious and time-consuming processes so that they can save their resources, in addition to having equipment that is backed up by statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are links that have given me more background information on n-grams and text classification, or have been useful in helping me write code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://stats.stackexchange.com/questions/25696/regarding-using-bigram-n-gram-model-to-build-feature-vector-for-text-document\n",
    "2. http://text-analytics101.rxnlp.com/2014/11/what-are-n-grams.html\n",
    "3. https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "4. https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.merge.html\n",
    "5. https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.groupby.html\n",
    "6. https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "7. https://medium.com/jbennetcodes/how-to-rewrite-your-sql-queries-in-pandas-and-more-149d341fc53e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
