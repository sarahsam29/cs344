

In class, we discussed how it is impossible to have a single-layered neural network for the XOR function. This is because XOR function 
is not linearly separable and therefore there is no linear model that can successfully learn the XOR function. Perceptrons, specifically,
include a single layer of input units (and one bias unit), and a single output unit. This structure makes it a “feed-forward” type of 
network, which means that the process of obtaining an output occurs in one direction from the single input layer to the output. Moreover, 
there is no communication between units in the input layer. This type of architecture is limiting because XOR functions cannot be sort 
with a single line since they are linearly separable. 

But a multilayered network, which is also a “feed-forward”, can learn XOR function since it can add a second layer called the hidden 
layer.Because of its complex architecture, the multilayered perceptron is able to achieve the non-linear separation that an XOR function 
requires. Moreover, if you can get around the “feed-forward” type of network, there is a way of learning the XOR function. This algorithm
is known as backpropagation. This method works by comparing the actual output to the expected output, then backtracks to adjusts the 
weights so that it reduces the error size by a small amount. This forward/backward propagation happens several times on each input 
combination until the network is able to predict the expected output accurately. 

References: 
https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b
