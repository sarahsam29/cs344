Sarah Samuel
CS 344, Lab10
April 20, 2019

Exercise 10.2

1. Agrad modifies the learning rate for each coefficient in the model, which lowers the effective learning rate. Agrad is used 
for convex problems. 


2. Solutions to Task 1 - 4:

    Task 1 :

    def normalize_linear_scale(examples_dataframe):
  
      processed_features = pd.DataFrame()
      processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
      processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
      processed_features["housing_median_age"] = linear_scale(examples_dataframe["housing_median_age"])
      processed_features["total_rooms"] = linear_scale(examples_dataframe["total_rooms"])
      processed_features["total_bedrooms"] = linear_scale(examples_dataframe["total_bedrooms"])
      processed_features["population"] = linear_scale(examples_dataframe["population"])
      processed_features["households"] = linear_scale(examples_dataframe["households"])
      processed_features["median_income"] = linear_scale(examples_dataframe["median_income"])
      processed_features["rooms_per_person"] = linear_scale(examples_dataframe["rooms_per_person"])
      return processed_features

    normalized_dataframe = normalize_linear_scale(preprocess_features(california_housing_dataframe))
    normalized_training_examples = normalized_dataframe.head(12000)
    normalized_validation_examples = normalized_dataframe.tail(5000)

    _ = train_nn_regression_model(
      my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.005),
      steps=2000,
      batch_size=50,
      hidden_units=[10, 10],
      training_examples=normalized_training_examples,
      training_targets=training_targets,
      validation_examples=normalized_validation_examples,
      validation_targets=validation_targets)

    Final RMSE of training set: 70.92
    Final RMSE of validation set: 71.13

  Task 2: 
  
  Agrad Solutions: 
    _, adagrad_training_losses, adagrad_validation_losses = train_nn_regression_model(
        my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.5),
        steps=500,
        batch_size=120,
        hidden_units=[10, 8],
        training_examples=normalized_training_examples,
        training_targets=training_targets,
        validation_examples=normalized_validation_examples,
        validation_targets=validation_targets)

    Final RMSE on training set: 69.03
    Final RMSE on validation set: 69.29

  Adam solutions:
    adam_training_losses, adam_validation_losses = train_nn_regression_model(
        my_optimizer=tf.train.AdamOptimizer(learning_rate=0.009),
        steps=500,
        batch_size=120,
        hidden_units=[10, 8],
        training_examples=normalized_training_examples,
        training_targets=training_targets,
        validation_examples=normalized_validation_examples,
        validation_targets=validation_targets)
        
    Final RMSE on training set: 69.66
    Final RMSE on validation set: 69.66

  Task 3: 
    def normalize(examples_dataframe):
      processed_features = pd.DataFrame()

      processed_features["households"] = log_normalize(examples_dataframe["households"])
      processed_features["median_income"] = log_normalize(examples_dataframe["median_income"])
      processed_features["total_bedrooms"] = log_normalize(examples_dataframe["total_bedrooms"])

      processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
      processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
      processed_features["housing_median_age"] = linear_scale(examples_dataframe["housing_median_age"])

      processed_features["population"] = linear_scale(clip(examples_dataframe["population"], 0, 5000))
      processed_features["rooms_per_person"] = linear_scale(clip(examples_dataframe["rooms_per_person"], 0, 5))
      processed_features["total_rooms"] = linear_scale(clip(examples_dataframe["total_rooms"], 0, 10000))

      return processed_features

    normalized_dataframe = normalize(preprocess_features(california_housing_dataframe))
    normalized_training_examples = normalized_dataframe.head(12000)
    normalized_validation_examples = normalized_dataframe.tail(5000)

    _ = train_nn_regression_model(
        my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0007),
        steps=1000,
        batch_size=60,
        hidden_units=[10, 10],
        training_examples=normalized_training_examples,
        training_targets=training_targets,
        validation_examples=normalized_validation_examples,
        validation_targets=validation_targets)

    Final RMSE on training data: 69.21
    Final RMSE on validation data: 68.94

  Task 4: 

    def location_location_location(examples_dataframe):

      processed_features = pd.DataFrame()
      processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
      processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
      return processed_features

    lll_dataframe = location_location_location(preprocess_features(california_housing_dataframe))
    lll_training_examples = lll_dataframe.head(12000)
    lll_validation_examples = lll_dataframe.tail(5000)

    _ = train_nn_regression_model(
        my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.05),
        steps=800,
        batch_size=70,
        hidden_units=[10, 10, 5, 5, 5],
        training_examples=lll_training_examples,
        training_targets=training_targets,
        validation_examples=lll_validation_examples,
        validation_targets=validation_targets)

    Final RMSE on training set: 98.28
    Final RMSE on validation set: 98.53
    
    
