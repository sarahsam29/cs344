Sarah Samuel
lab07, Exercise 4
March 15, 2019

a. Solutions to task 1-5:

  Task 1:
    The first thing I noticed what the max size for rooms_per_person in both the training and validation sets- it
    seems odd to have 55 people, and 18 rooms per person. I also noticed that the max population for training and 
    validation sets are 35682 and 28566, respectively, which seems excessively high for the population of a single
    block, especially considering that this data reflects 1990 data). 
  
  Task 2: 
    By comparing the visuals from the validation and training sets, we should observe that the training data and 
    validation data have distributions that are unequally split.
    
  Task 3: 
    This is where randomizing the data will come in handy because you can make sure that there is no order present
    in the data before you split it into training and validation sets.
  
  Task 4: (I was quite confused as to what to do, so I looked at the solutions and tried to understand what was happening)

      def train_model(
        learning_rate,
        steps,
        batch_size,
        training_examples,
        training_targets,
        validation_examples,
        validation_targets):
       periods = 10
      steps_per_period = steps / periods

      # Create a linear regressor object.
      my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
      my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
      linear_regressor = tf.estimator.LinearRegressor(
          feature_columns=construct_feature_columns(training_examples),
          optimizer=my_optimizer
      )

      # Create input functions.
      training_input_fn = lambda: my_input_fn(
          training_examples, 
          training_targets["median_house_value"], 
          batch_size=batch_size)
      predict_training_input_fn = lambda: my_input_fn(
          training_examples, 
          training_targets["median_house_value"], 
          num_epochs=1, 
          shuffle=False)
      predict_validation_input_fn = lambda: my_input_fn(
          validation_examples, validation_targets["median_house_value"], 
          num_epochs=1, 
          shuffle=False)

      # Train the model, but do so inside a loop so that we can periodically assess  loss metrics.
      print("Training model...")
      print("RMSE (on training data):")
      training_rmse = []
      validation_rmse = []
      for period in range (0, periods):
        # Train the model, starting from the prior state.
        linear_regressor.train(
            input_fn=training_input_fn,
            steps=steps_per_period,
        )
        # Take a break and compute predictions.
        training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)
        training_predictions = np.array([item['predictions'][0] for item in training_predictions])

        validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)
        validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])


        # Compute training and validation loss.
        training_root_mean_squared_error = math.sqrt(
            metrics.mean_squared_error(training_predictions, training_targets))
        validation_root_mean_squared_error = math.sqrt(
            metrics.mean_squared_error(validation_predictions, validation_targets))
        # Occasionally print the current loss.
        print("  period %02d : %0.2f" % (period, training_root_mean_squared_error))
        # Add the loss metrics from this period to our list.
        training_rmse.append(training_root_mean_squared_error)
        validation_rmse.append(validation_root_mean_squared_error)
      print("Model training finished.")

      # Output a graph of loss metrics over periods.
      plt.ylabel("RMSE")
      plt.xlabel("Periods")
      plt.title("Root Mean Squared Error vs. Periods")
      plt.tight_layout()
      plt.plot(training_rmse, label="training")
      plt.plot(validation_rmse, label="validation")
      plt.legend()

      return linear_regressor
  
    Task 5: 
         california_housing_test_data = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv", sep=",")

        test_examples = preprocess_features(california_housing_test_data)
        test_targets = preprocess_targets(california_housing_test_data)

        predict_test_input_fn = lambda: my_input_fn(
              test_examples, 
              test_targets["median_house_value"], 
              num_epochs=1, 
              shuffle=False)

        test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)
        test_predictions = np.array([item['predictions'][0] for item in test_predictions])

        root_mean_squared_error = math.sqrt(
            metrics.mean_squared_error(test_predictions, test_targets))

        print("Final RMSE (on test data): %0.2f" % root_mean_squared_error)
        
        
   b. The training dataset is the an actual sample of the data that is used to fit a model. This is the data that 
   the model learns trends from. The validation dataset is harder for me to understand fully, but it is another sample
   of the data that is used to provide an unbiased evaluation of the model that fits the training dataset, as well as
   manipulate hyperparameters. The test dataset is used in the very end to justly evaluate  the final fit of the training
   dataset. The test dataset is only used on a completely trained model by the means of both the training and validation datasets.  
   
   
